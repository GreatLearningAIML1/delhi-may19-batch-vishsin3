{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "fHpCNRv1OB5-",
    "outputId": "ebb29f17-9d9d-4cce-cc51-d0ed5282e5dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "FhllFLyKOB6N",
    "outputId": "2e93ad17-0da5-4cc4-fdbc-c790c7493959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/edrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('/edrive/My Drive/residency6/prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "7K8pWsNQOB6X",
    "outputId": "247c2c4b-2440-4593-b395-a5682e3961d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open  ...         low        high     volume\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-zZ54DaiAau"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e0nEbpNAiCzf",
    "outputId": "5d30363b-e7bd-4901-8403-9ff68b8e8c08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851264, 7)"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "lh_6spSKOB6e",
    "outputId": "7167abbc-469a-4e3b-d495-d20d7b6d9e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSV13kRdjMKB"
   },
   "outputs": [],
   "source": [
    "dataset2=dataset1.drop([\"date\",\"symbol\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "cCwwVwmSjd0v",
    "outputId": "bdc42897-2e5a-425b-9bae-1cfb70a82730"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 139,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "cac894c6-822c-4c48-8f20-63e3b0723dae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 140,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "dataset3=dataset2.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dXhlqPeQlGmB",
    "outputId": "1a4fa1ba-9249-4617-fabd-18e01f4e6e0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "qvmgO_RQASXY",
    "outputId": "49f5c393-5bbc-4c92-db3b-a3f2eabeb38d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset3[\"volume\"]=dataset3[\"volume\"]/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "b-y_O1GgAlCR",
    "outputId": "b445c15c-b996-43e9-93a1-88f55b362ac6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2.3864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2.4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1.4086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high  volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2.1636\n",
       "1  125.239998  119.980003  119.940002  125.540001  2.3864\n",
       "2  116.379997  114.949997  114.930000  119.739998  2.4895\n",
       "3  115.480003  116.620003  113.500000  117.440002  2.0063\n",
       "4  117.010002  114.970001  114.089996  117.330002  1.4086"
      ]
     },
     "execution_count": 200,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fsV8swwmOz1"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsiFlN1yl1jM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeZ8M1LToBBW"
   },
   "outputs": [],
   "source": [
    "x=dataset3.drop([\"volume\"],axis=1)\n",
    "y=dataset3[\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gE9ZY_l4n4UB"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train=np.array(x_train).astype(np.float32)\n",
    "x_test=np.array(x_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSzUH6zqqR20"
   },
   "outputs": [],
   "source": [
    "y_train=np.array(y_train).astype(np.float32)\n",
    "y_test=np.array(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z-664IZRqlaL",
    "outputId": "a6679e1c-a443-4ad5-b7b1-4411017434a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 207,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e8gLFNor_ueJ",
    "outputId": "54a78938-f849-4c7a-ca8a-df3eee62d559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 208,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sen2l5e3_uTG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4MILegIrZe0"
   },
   "outputs": [],
   "source": [
    "x_train1=normalize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "z0hKCyLnrZRE",
    "outputId": "1d3de73d-09e9-40aa-c0c9-521dcb781161"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49503326, 0.50456154, 0.49442136, 0.5058727 ],\n",
       "       [0.49014246, 0.5128518 , 0.48143727, 0.5147442 ],\n",
       "       [0.49802995, 0.50234187, 0.495874  , 0.5037139 ],\n",
       "       ...,\n",
       "       [0.49995428, 0.5016043 , 0.49572614, 0.5026871 ],\n",
       "       [0.49740046, 0.5029231 , 0.49371874, 0.50586843],\n",
       "       [0.50012493, 0.5005309 , 0.49594367, 0.5033725 ]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGjlNmc-rY9D"
   },
   "outputs": [],
   "source": [
    "x_test1=normalize(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62"
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A"
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "#We are initializing weights and Bias with Zero\n",
    "w = tf.zeros(shape=(4,1))\n",
    "b = tf.zeros(shape=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F"
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "def prediction(x1, w, b):\n",
    "    \n",
    "    xw_matmul = tf.matmul(x1, w)\n",
    "    y_out = tf.add(xw_matmul, b)\n",
    "    \n",
    "    return y_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M"
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss(y_actual, y_predicted):\n",
    "    \n",
    "    diff = y_actual - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U"
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "def train(x1, y_actual, w, b, learning_rate=0.01):\n",
    "    \n",
    "    #Record mathematical operations on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        t.watch([w,b])\n",
    "        \n",
    "        current_prediction = prediction(x1, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    \n",
    "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    \n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e"
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Train loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DVvgj7eQOB7f",
    "outputId": "255c0214-edbd-4e43-e422-e6b57ebd6287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current Loss on iteration', 0, 205.55208)\n",
      "('Current Loss on iteration', 1, 203.65453)\n",
      "('Current Loss on iteration', 2, 201.90575)\n",
      "('Current Loss on iteration', 3, 200.29407)\n",
      "('Current Loss on iteration', 4, 198.80875)\n",
      "('Current Loss on iteration', 5, 197.43985)\n",
      "('Current Loss on iteration', 6, 196.1783)\n",
      "('Current Loss on iteration', 7, 195.01566)\n",
      "('Current Loss on iteration', 8, 193.94412)\n",
      "('Current Loss on iteration', 9, 192.95662)\n",
      "('Current Loss on iteration', 10, 192.04655)\n",
      "('Current Loss on iteration', 11, 191.2078)\n",
      "('Current Loss on iteration', 12, 190.43483)\n",
      "('Current Loss on iteration', 13, 189.72244)\n",
      "('Current Loss on iteration', 14, 189.06592)\n",
      "('Current Loss on iteration', 15, 188.46085)\n",
      "('Current Loss on iteration', 16, 187.90323)\n",
      "('Current Loss on iteration', 17, 187.38933)\n",
      "('Current Loss on iteration', 18, 186.91573)\n",
      "('Current Loss on iteration', 19, 186.47922)\n",
      "('Current Loss on iteration', 20, 186.07695)\n",
      "('Current Loss on iteration', 21, 185.7062)\n",
      "('Current Loss on iteration', 22, 185.36455)\n",
      "('Current Loss on iteration', 23, 185.04967)\n",
      "('Current Loss on iteration', 24, 184.75948)\n",
      "('Current Loss on iteration', 25, 184.49202)\n",
      "('Current Loss on iteration', 26, 184.24554)\n",
      "('Current Loss on iteration', 27, 184.01842)\n",
      "('Current Loss on iteration', 28, 183.80905)\n",
      "('Current Loss on iteration', 29, 183.61612)\n",
      "('Current Loss on iteration', 30, 183.43832)\n",
      "('Current Loss on iteration', 31, 183.27444)\n",
      "('Current Loss on iteration', 32, 183.12344)\n",
      "('Current Loss on iteration', 33, 182.98425)\n",
      "('Current Loss on iteration', 34, 182.85599)\n",
      "('Current Loss on iteration', 35, 182.73776)\n",
      "('Current Loss on iteration', 36, 182.62883)\n",
      "('Current Loss on iteration', 37, 182.52841)\n",
      "('Current Loss on iteration', 38, 182.43588)\n",
      "('Current Loss on iteration', 39, 182.3506)\n",
      "('Current Loss on iteration', 40, 182.27202)\n",
      "('Current Loss on iteration', 41, 182.19957)\n",
      "('Current Loss on iteration', 42, 182.13283)\n",
      "('Current Loss on iteration', 43, 182.07132)\n",
      "('Current Loss on iteration', 44, 182.01462)\n",
      "('Current Loss on iteration', 45, 181.96236)\n",
      "('Current Loss on iteration', 46, 181.9142)\n",
      "('Current Loss on iteration', 47, 181.86983)\n",
      "('Current Loss on iteration', 48, 181.82892)\n",
      "('Current Loss on iteration', 49, 181.79123)\n",
      "('Current Loss on iteration', 50, 181.7565)\n",
      "('Current Loss on iteration', 51, 181.72447)\n",
      "('Current Loss on iteration', 52, 181.69496)\n",
      "('Current Loss on iteration', 53, 181.66777)\n",
      "('Current Loss on iteration', 54, 181.6427)\n",
      "('Current Loss on iteration', 55, 181.61963)\n",
      "('Current Loss on iteration', 56, 181.59834)\n",
      "('Current Loss on iteration', 57, 181.57872)\n",
      "('Current Loss on iteration', 58, 181.56062)\n",
      "('Current Loss on iteration', 59, 181.544)\n",
      "('Current Loss on iteration', 60, 181.52863)\n",
      "('Current Loss on iteration', 61, 181.51448)\n",
      "('Current Loss on iteration', 62, 181.50143)\n",
      "('Current Loss on iteration', 63, 181.48941)\n",
      "('Current Loss on iteration', 64, 181.47833)\n",
      "('Current Loss on iteration', 65, 181.46812)\n",
      "('Current Loss on iteration', 66, 181.45871)\n",
      "('Current Loss on iteration', 67, 181.45004)\n",
      "('Current Loss on iteration', 68, 181.44205)\n",
      "('Current Loss on iteration', 69, 181.4347)\n",
      "('Current Loss on iteration', 70, 181.4279)\n",
      "('Current Loss on iteration', 71, 181.42165)\n",
      "('Current Loss on iteration', 72, 181.4159)\n",
      "('Current Loss on iteration', 73, 181.41057)\n",
      "('Current Loss on iteration', 74, 181.40567)\n",
      "('Current Loss on iteration', 75, 181.40117)\n",
      "('Current Loss on iteration', 76, 181.397)\n",
      "('Current Loss on iteration', 77, 181.39316)\n",
      "('Current Loss on iteration', 78, 181.38965)\n",
      "('Current Loss on iteration', 79, 181.38638)\n",
      "('Current Loss on iteration', 80, 181.38338)\n",
      "('Current Loss on iteration', 81, 181.38062)\n",
      "('Current Loss on iteration', 82, 181.37808)\n",
      "('Current Loss on iteration', 83, 181.37573)\n",
      "('Current Loss on iteration', 84, 181.37357)\n",
      "('Current Loss on iteration', 85, 181.37157)\n",
      "('Current Loss on iteration', 86, 181.36974)\n",
      "('Current Loss on iteration', 87, 181.36804)\n",
      "('Current Loss on iteration', 88, 181.36649)\n",
      "('Current Loss on iteration', 89, 181.36505)\n",
      "('Current Loss on iteration', 90, 181.36371)\n",
      "('Current Loss on iteration', 91, 181.3625)\n",
      "('Current Loss on iteration', 92, 181.36137)\n",
      "('Current Loss on iteration', 93, 181.36032)\n",
      "('Current Loss on iteration', 94, 181.35936)\n",
      "('Current Loss on iteration', 95, 181.35847)\n",
      "('Current Loss on iteration', 96, 181.35767)\n",
      "('Current Loss on iteration', 97, 181.35692)\n",
      "('Current Loss on iteration', 98, 181.35623)\n",
      "('Current Loss on iteration', 99, 181.3556)\n"
     ]
    }
   ],
   "source": [
    "#Train for 100 Steps\n",
    "for i in range(100):\n",
    "    \n",
    "    w, b = train(x_train1,y_train, w, b, learning_rate=0.01)\n",
    "    print('Current Loss on iteration', i, \n",
    "          loss(y_train, prediction(x_train1, w, b)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r",
    "outputId": "b7d5e50c-91f8-42bc-84c7-ed71d74afe0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(1)])"
      ]
     },
     "execution_count": 219,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "qOOEtF_uzotc",
    "outputId": "f0c91e01-49a9-42cb-a950-5a71ea80b6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=14218, shape=(4, 1), dtype=float32, numpy=\n",
       "array([[1.25822  ],\n",
       "       [1.2619466],\n",
       "       [1.244955 ],\n",
       "       [1.2729903]], dtype=float32)>"
      ]
     },
     "execution_count": 220,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vhDtOv5UOB7x",
    "outputId": "9b43bb79-bf36-41cf-906f-db7285d42a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1)])"
      ]
     },
     "execution_count": 221,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PxeEg2-ozqq4",
    "outputId": "f1b760d0-734c-4f4a-e6fd-8ff8d063b8b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=14221, shape=(1,), dtype=float32, numpy=array([2.5191956], dtype=float32)>"
      ]
     },
     "execution_count": 222,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gKGvUWahcihp",
    "outputId": "9d61bf45-01e7-4bd6-8dea-dba475b84e76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 223,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x_test1[0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCgS2S0K2Cal"
   },
   "outputs": [],
   "source": [
    "pred=prediction(x_test1[0:1], w, b).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u_3x8_1S2SDp",
    "outputId": "ec9b0cd6-3cc2-4b79-bc3c-05d848d7bae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.0383167]], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oSdFMo542Ydo",
    "outputId": "dd29433c-a80a-4b8e-bb6b-c05f93204095"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.263332"
      ]
     },
     "execution_count": 227,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_test[0:1],prediction(x_test1[0:1], w, b)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OQCci83p2mp8",
    "outputId": "8a10e754-88cb-4950-9a9d-bf7afe43709d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUiWAFtGBUpd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "IJr5dYnocihm",
    "outputId": "d72c1546-ca22-4623-bd41-11be1fd960ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 230,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "arVZ5HIW-jJf",
    "outputId": "76a55479-5478-445b-c543-fcd3d36b9078"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 233,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['Species']= label_encoder.fit_transform(df['Species']) \n",
    "  \n",
    "df['Species'].unique() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "HNlOzu4pBnwN",
    "outputId": "ede36ec6-5df2-4248-e41d-dd22752e7d9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0   1            5.1           3.5            1.4           0.2        0\n",
       "1   2            4.9           3.0            1.4           0.2        0\n",
       "2   3            4.7           3.2            1.3           0.2        0\n",
       "3   4            4.6           3.1            1.5           0.2        0\n",
       "4   5            5.0           3.6            1.4           0.2        0"
      ]
     },
     "execution_count": 234,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyQ9kZfW95YT"
   },
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(df,columns=[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "7nenNq7VCIMz",
    "outputId": "3fdba459-7470-46c0-a305-19be4081b6ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species_0</th>\n",
       "      <th>Species_1</th>\n",
       "      <th>Species_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  ...  Species_0  Species_1  Species_2\n",
       "0   1            5.1           3.5  ...          1          0          0\n",
       "1   2            4.9           3.0  ...          1          0          0\n",
       "2   3            4.7           3.2  ...          1          0          0\n",
       "3   4            4.6           3.1  ...          1          0          0\n",
       "4   5            5.0           3.6  ...          1          0          0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT6RsI6--Kx9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUYF8ZFs-XSQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQoLMucihj"
   },
   "outputs": [],
   "source": [
    "x_df1=df1.drop([\"Id\",\"Species_0\",\"Species_1\",\"Species_2\"],axis=1)\n",
    "y_df1=df1[[\"Species_0\",\"Species_1\",\"Species_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-mK6uCcE8q4"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(x_df1,y_df1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "baTTwQPyISot",
    "outputId": "71ad0ecb-21e8-44fd-dd03-7d19581bc3d0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "122            7.7           2.8            6.7           2.0\n",
       "13             4.3           3.0            1.1           0.1\n",
       "32             5.2           4.1            1.5           0.1\n",
       "21             5.1           3.7            1.5           0.4\n",
       "40             5.0           3.5            1.3           0.3\n",
       "99             5.7           2.8            4.1           1.3\n",
       "7              5.0           3.4            1.5           0.2\n",
       "1              4.9           3.0            1.4           0.2\n",
       "84             5.4           3.0            4.5           1.5\n",
       "89             5.5           2.5            4.0           1.3\n",
       "112            6.8           3.0            5.5           2.1\n",
       "125            7.2           3.2            6.0           1.8\n",
       "35             5.0           3.2            1.2           0.2\n",
       "147            6.5           3.0            5.2           2.0\n",
       "42             4.4           3.2            1.3           0.2\n",
       "44             5.1           3.8            1.9           0.4\n",
       "94             5.6           2.7            4.2           1.3\n",
       "110            6.5           3.2            5.1           2.0\n",
       "85             6.0           3.4            4.5           1.6\n",
       "67             5.8           2.7            4.1           1.0\n",
       "75             6.6           3.0            4.4           1.4\n",
       "60             5.0           2.0            3.5           1.0\n",
       "115            6.4           3.2            5.3           2.3\n",
       "71             6.1           2.8            4.0           1.3\n",
       "77             6.7           3.0            5.0           1.7\n",
       "95             5.7           3.0            4.2           1.2\n",
       "113            5.7           2.5            5.0           2.0\n",
       "68             6.2           2.2            4.5           1.5\n",
       "142            5.8           2.7            5.1           1.9\n",
       "39             5.1           3.4            1.5           0.2\n",
       "..             ...           ...            ...           ...\n",
       "108            6.7           2.5            5.8           1.8\n",
       "136            6.3           3.4            5.6           2.4\n",
       "140            6.7           3.1            5.6           2.4\n",
       "29             4.7           3.2            1.6           0.2\n",
       "14             5.8           4.0            1.2           0.2\n",
       "2              4.7           3.2            1.3           0.2\n",
       "57             4.9           2.4            3.3           1.0\n",
       "59             5.2           2.7            3.9           1.4\n",
       "123            6.3           2.7            4.9           1.8\n",
       "74             6.4           2.9            4.3           1.3\n",
       "66             5.6           3.0            4.5           1.5\n",
       "11             4.8           3.4            1.6           0.2\n",
       "27             5.2           3.5            1.5           0.2\n",
       "149            5.9           3.0            5.1           1.8\n",
       "41             4.5           2.3            1.3           0.3\n",
       "144            6.7           3.3            5.7           2.5\n",
       "47             4.6           3.2            1.4           0.2\n",
       "127            6.1           3.0            4.9           1.8\n",
       "56             6.3           3.3            4.7           1.6\n",
       "8              4.4           2.9            1.4           0.2\n",
       "55             5.7           2.8            4.5           1.3\n",
       "131            7.9           3.8            6.4           2.0\n",
       "118            7.7           2.6            6.9           2.3\n",
       "0              5.1           3.5            1.4           0.2\n",
       "3              4.6           3.1            1.5           0.2\n",
       "128            6.4           2.8            5.6           2.1\n",
       "139            6.9           3.1            5.4           2.1\n",
       "117            7.7           3.8            6.7           2.2\n",
       "104            6.5           3.0            5.8           2.2\n",
       "6              4.6           3.4            1.4           0.3\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "#add model layers\n",
    "model.add(tf.keras.layers.Dense(3, input_shape=(4,),activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFfU5Ow_K1GI"
   },
   "outputs": [],
   "source": [
    "#complie the model \n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwTz6z9fK04b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4qLEdHPscihx",
    "outputId": "e34189fa-11ae-40f3-f54d-a0e75080a568"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 12:12:37.477353 140144041236352 training.py:503] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n",
      "W1103 12:12:37.558943 140144041236352 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 7.1625 - acc: 0.3417 - val_loss: 6.7475 - val_acc: 0.3000\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 5.8115 - acc: 0.3417 - val_loss: 5.5681 - val_acc: 0.3000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 135us/sample - loss: 4.7966 - acc: 0.3417 - val_loss: 4.7078 - val_acc: 0.3000\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 4.0487 - acc: 0.3417 - val_loss: 4.1036 - val_acc: 0.3000\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 3.5364 - acc: 0.3417 - val_loss: 3.6711 - val_acc: 0.2333\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 3.1884 - acc: 0.2583 - val_loss: 3.3350 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 128us/sample - loss: 2.9076 - acc: 0.1167 - val_loss: 3.0189 - val_acc: 0.0667\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 2.6424 - acc: 0.0583 - val_loss: 2.7108 - val_acc: 0.1667\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 2.3726 - acc: 0.1417 - val_loss: 2.4105 - val_acc: 0.1333\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 126us/sample - loss: 2.1112 - acc: 0.1333 - val_loss: 2.1008 - val_acc: 0.1333\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 128us/sample - loss: 1.8548 - acc: 0.1167 - val_loss: 1.8034 - val_acc: 0.1333\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 1.6079 - acc: 0.1500 - val_loss: 1.5582 - val_acc: 0.0667\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.4203 - acc: 0.1000 - val_loss: 1.3766 - val_acc: 0.1000\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 1.2886 - acc: 0.1333 - val_loss: 1.2549 - val_acc: 0.0667\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 1.2043 - acc: 0.0750 - val_loss: 1.1770 - val_acc: 0.0667\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 1.1513 - acc: 0.0667 - val_loss: 1.1272 - val_acc: 0.0333\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 1.1133 - acc: 0.0583 - val_loss: 1.0909 - val_acc: 0.0667\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 1.0885 - acc: 0.0667 - val_loss: 1.0628 - val_acc: 0.0667\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 1.0741 - acc: 0.0417 - val_loss: 1.0369 - val_acc: 0.1333\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 1.0394 - acc: 0.1167 - val_loss: 1.0126 - val_acc: 0.1667\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 1.0195 - acc: 0.2417 - val_loss: 0.9908 - val_acc: 0.2333\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.9945 - acc: 0.2583 - val_loss: 0.9685 - val_acc: 0.3667\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.9772 - acc: 0.2750 - val_loss: 0.9463 - val_acc: 0.4333\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9575 - acc: 0.3500 - val_loss: 0.9279 - val_acc: 0.4333\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.9422 - acc: 0.3500 - val_loss: 0.9111 - val_acc: 0.4667\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.9240 - acc: 0.3417 - val_loss: 0.8952 - val_acc: 0.4667\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9090 - acc: 0.3833 - val_loss: 0.8789 - val_acc: 0.4333\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8923 - acc: 0.4000 - val_loss: 0.8665 - val_acc: 0.4333\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 112us/sample - loss: 0.8794 - acc: 0.3917 - val_loss: 0.8537 - val_acc: 0.4333\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 184us/sample - loss: 0.8659 - acc: 0.3583 - val_loss: 0.8411 - val_acc: 0.4333\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 187us/sample - loss: 0.8573 - acc: 0.4000 - val_loss: 0.8257 - val_acc: 0.4667\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8406 - acc: 0.4000 - val_loss: 0.8151 - val_acc: 0.4667\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8363 - acc: 0.3750 - val_loss: 0.8016 - val_acc: 0.5000\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 166us/sample - loss: 0.8190 - acc: 0.4500 - val_loss: 0.7929 - val_acc: 0.4667\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.8179 - acc: 0.3917 - val_loss: 0.7823 - val_acc: 0.4667\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.7995 - acc: 0.3833 - val_loss: 0.7719 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.7897 - acc: 0.4167 - val_loss: 0.7635 - val_acc: 0.4333\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.7881 - acc: 0.3917 - val_loss: 0.7534 - val_acc: 0.5333\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.7786 - acc: 0.5417 - val_loss: 0.7486 - val_acc: 0.4667\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.7670 - acc: 0.4167 - val_loss: 0.7403 - val_acc: 0.4667\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7596 - acc: 0.4333 - val_loss: 0.7293 - val_acc: 0.5000\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7536 - acc: 0.4333 - val_loss: 0.7220 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 135us/sample - loss: 0.7450 - acc: 0.4167 - val_loss: 0.7142 - val_acc: 0.5333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.7363 - acc: 0.4833 - val_loss: 0.7080 - val_acc: 0.5000\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 0.7297 - acc: 0.4417 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7267 - acc: 0.4750 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7222 - acc: 0.4417 - val_loss: 0.6882 - val_acc: 0.5333\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 127us/sample - loss: 0.7127 - acc: 0.4833 - val_loss: 0.6829 - val_acc: 0.5333\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.7045 - acc: 0.5083 - val_loss: 0.6779 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.7001 - acc: 0.4917 - val_loss: 0.6729 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.6923 - acc: 0.4417 - val_loss: 0.6668 - val_acc: 0.5333\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.6893 - acc: 0.4750 - val_loss: 0.6613 - val_acc: 0.5667\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 0.6820 - acc: 0.5417 - val_loss: 0.6577 - val_acc: 0.5000\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6813 - acc: 0.4417 - val_loss: 0.6513 - val_acc: 0.5667\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 117us/sample - loss: 0.6748 - acc: 0.5417 - val_loss: 0.6470 - val_acc: 0.5667\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 134us/sample - loss: 0.6739 - acc: 0.5417 - val_loss: 0.6423 - val_acc: 0.5667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.6650 - acc: 0.4750 - val_loss: 0.6376 - val_acc: 0.5667\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.6601 - acc: 0.5417 - val_loss: 0.6343 - val_acc: 0.5667\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 112us/sample - loss: 0.6547 - acc: 0.5417 - val_loss: 0.6319 - val_acc: 0.5000\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.6583 - acc: 0.4667 - val_loss: 0.6267 - val_acc: 0.5667\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.6559 - acc: 0.5250 - val_loss: 0.6217 - val_acc: 0.5667\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 140us/sample - loss: 0.6445 - acc: 0.5417 - val_loss: 0.6181 - val_acc: 0.5667\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 155us/sample - loss: 0.6394 - acc: 0.6000 - val_loss: 0.6163 - val_acc: 0.5000\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.6352 - acc: 0.5250 - val_loss: 0.6132 - val_acc: 0.5333\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.6415 - acc: 0.5000 - val_loss: 0.6079 - val_acc: 0.5667\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.6303 - acc: 0.6000 - val_loss: 0.6062 - val_acc: 0.5667\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.6275 - acc: 0.5917 - val_loss: 0.6067 - val_acc: 0.5333\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.6248 - acc: 0.5500 - val_loss: 0.6027 - val_acc: 0.5333\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.6207 - acc: 0.5583 - val_loss: 0.6004 - val_acc: 0.5333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.6159 - acc: 0.5167 - val_loss: 0.5931 - val_acc: 0.6000\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.6124 - acc: 0.5750 - val_loss: 0.5902 - val_acc: 0.6000\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.6172 - acc: 0.5417 - val_loss: 0.5852 - val_acc: 0.6000\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.6093 - acc: 0.6500 - val_loss: 0.5862 - val_acc: 0.5667\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.6051 - acc: 0.5917 - val_loss: 0.5816 - val_acc: 0.5667\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.6027 - acc: 0.5833 - val_loss: 0.5782 - val_acc: 0.6000\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.5987 - acc: 0.6083 - val_loss: 0.5776 - val_acc: 0.5667\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.5966 - acc: 0.5917 - val_loss: 0.5745 - val_acc: 0.5667\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 135us/sample - loss: 0.5977 - acc: 0.5833 - val_loss: 0.5692 - val_acc: 0.6333\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.5923 - acc: 0.6417 - val_loss: 0.5707 - val_acc: 0.5667\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.5896 - acc: 0.6000 - val_loss: 0.5700 - val_acc: 0.5667\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.5909 - acc: 0.6083 - val_loss: 0.5665 - val_acc: 0.6000\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.5839 - acc: 0.6000 - val_loss: 0.5643 - val_acc: 0.6000\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.5914 - acc: 0.6083 - val_loss: 0.5589 - val_acc: 0.6000\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.5786 - acc: 0.6333 - val_loss: 0.5548 - val_acc: 0.6333\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.5761 - acc: 0.6667 - val_loss: 0.5524 - val_acc: 0.6333\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.5728 - acc: 0.6333 - val_loss: 0.5500 - val_acc: 0.6667\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.5720 - acc: 0.6500 - val_loss: 0.5479 - val_acc: 0.6667\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 0.5688 - acc: 0.6500 - val_loss: 0.5469 - val_acc: 0.6333\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.5671 - acc: 0.6667 - val_loss: 0.5437 - val_acc: 0.6667\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.5664 - acc: 0.6667 - val_loss: 0.5428 - val_acc: 0.6333\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.5635 - acc: 0.6667 - val_loss: 0.5402 - val_acc: 0.6333\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 135us/sample - loss: 0.5592 - acc: 0.6500 - val_loss: 0.5382 - val_acc: 0.6333\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.5576 - acc: 0.6833 - val_loss: 0.5361 - val_acc: 0.6667\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 212us/sample - loss: 0.5581 - acc: 0.6667 - val_loss: 0.5337 - val_acc: 0.6667\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.5539 - acc: 0.6583 - val_loss: 0.5327 - val_acc: 0.6667\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.5533 - acc: 0.7000 - val_loss: 0.5296 - val_acc: 0.7000\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.5555 - acc: 0.6250 - val_loss: 0.5274 - val_acc: 0.7000\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 0.5509 - acc: 0.6667 - val_loss: 0.5268 - val_acc: 0.7000\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 176us/sample - loss: 0.5455 - acc: 0.7417 - val_loss: 0.5246 - val_acc: 0.6667\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 0.5444 - acc: 0.7333 - val_loss: 0.5222 - val_acc: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7560160410>"
      ]
     },
     "execution_count": 270,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,          \n",
    "          validation_data=(X_test,Y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "colab_type": "code",
    "id": "GBgKZkhkcih6",
    "outputId": "2a945283-c46c-47ad-c24d-84b23a0c9b67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 12:13:47.075778 140144041236352 training.py:503] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <type 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01638924 0.41258666 0.5710241 ]\n",
      " [0.06581663 0.41024265 0.5239407 ]\n",
      " [0.83402747 0.11308996 0.05288265]\n",
      " [0.8790499  0.08117566 0.03977445]\n",
      " [0.7676849  0.1537056  0.0786095 ]\n",
      " [0.08641391 0.4135815  0.5000046 ]\n",
      " [0.18023391 0.40057522 0.41919088]\n",
      " [0.00970173 0.47882146 0.5114768 ]\n",
      " [0.8925524  0.07801453 0.02943304]\n",
      " [0.01112871 0.46715048 0.5217208 ]\n",
      " [0.8551923  0.09255672 0.0522509 ]\n",
      " [0.86167544 0.09120005 0.04712456]\n",
      " [0.10715192 0.41100764 0.4818404 ]\n",
      " [0.01194431 0.31206232 0.6759933 ]\n",
      " [0.02638213 0.34026235 0.63335556]\n",
      " [0.0244054  0.49772817 0.4778664 ]\n",
      " [0.07949421 0.3886858  0.53182   ]\n",
      " [0.03194223 0.4462017  0.52185607]\n",
      " [0.13347691 0.39605704 0.470466  ]\n",
      " [0.8251019  0.1092604  0.06563767]\n",
      " [0.798474   0.13373843 0.06778764]\n",
      " [0.8536469  0.09242607 0.05392697]\n",
      " [0.02584316 0.37901163 0.5951452 ]\n",
      " [0.01825771 0.3855388  0.5962035 ]\n",
      " [0.00449274 0.42124954 0.5742577 ]\n",
      " [0.15618122 0.4473282  0.39649063]\n",
      " [0.0191776  0.3829876  0.59783477]\n",
      " [0.8671931  0.08919588 0.04361101]\n",
      " [0.07441623 0.41731837 0.5082654 ]\n",
      " [0.8591408  0.0957996  0.04505964]]\n"
     ]
    }
   ],
   "source": [
    "#Model prediction\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#Print prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save('Lab6_internal_Iris.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R6_Internal_Lab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
